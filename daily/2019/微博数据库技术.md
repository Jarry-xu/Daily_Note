



## 1. 微博架构介绍

- 第一代架构： LAMP 架构，数据库使用的是 MyIsam(mysql默认的数据库引擎)，后台用的是 php，缓存为 Memcache。

- 第二代架构：随着应用规模的增长，衍生出的第二代架构对业务功能进行了模块化、服务化和组件化，后台系统从 php 替换为 Java，逐渐形成 SOA 架构，在很长一段时间支撑了微博平台的业务发展。

- 第三代架构：

  - ![亿级用户下的新浪微博平台架构](https://static001.infoq.cn/resource/image/ea/76/eaede3ca4a378072c41bc84afa549b76.png)

  对其进行正交分解可以得到如下的平台架构

  ![亿级用户下的新浪微博平台架构](https://static001.infoq.cn/resource/image/98/78/9806c57d77da35fe0d5c1c3cc06e6178.png)

### 1.1资源层介绍

我们重点看资源层，主要是数据模型的存储，包含通用的缓存资源 Redis 和 Memcached，以及持久化数据库存储 MySQL、HBase，或者分布式文件系统 TFS 以及 Sina S3 服务。

#### 资源层框架	

资源层的框架非常多，有封装 MySQL 与 HBase 的 Key-List DAL 中间件、有定制化的计数组件，有支持分布式 MC 与 Redis 的 Proxy，在这些方面业界有较多的经验分享，我在这里分享一下平台架构的对象库与 SSD Cache 组件。

##### 对象库

对象库支持便捷的序列化与反序列化微博中的对象数据：序列化时，将 JVM 内存中的对象序列化写入在 HBase 中并生成唯一的 ObjectID，当需要访问该对象时，通过 ObjectID 读取，对象库支持任意类型的对象，支持 PB、JSON、二进制序列化协议，微博中最大的应用场景将微博中引用的视频、图片、文章统一定义为对象，一共定义了几十种对象类型，并抽象出标准的对象元数据 Schema，对象的内容上传到对象存储系统（Sina S3）中，对象元数据中保存 Sina S3 的下载地址。

##### SSDCache

随着 SSD 硬盘的普及，优越的 IO 性能使其被越来越多地用于替换传统的 SATA 和 SAS 磁盘，常见的应用场景有三种：1）替换 MySQL 数据库的硬盘，目前社区还没有针对 SSD 优化的 MySQL 版本，即使这样，直接升级 SSD 硬盘也能带来 8 倍左右的 IOPS 提升；2）替换 Redis 的硬盘，提升其性能；3）用在 CDN 中，加快静态资源加载速度。

微博平台将 SSD 应用在分布式缓存场景中，将传统的 Redis/MC + Mysql 方式，扩展为 Redis/MC + SSD Cache + Mysql 方式，SSD Cache 作为 L2 缓存使用，第一降低了 MC/Redis 成本过高，容量小的问题，也解决了穿透 DB 带来的数据库访问压力。



## 2. 数据库业务划分

> **壹佰案例：**简单介绍下微博数据库的概况，除了MySQL、Redis还采用了哪些技术?
>
> **肖鹏：**微博数据库主流使用MySQL和Redis，辅助一定的HBase，并且由于缓存和队列和数据库的关联比较紧密，我们也是一起负责的，这样就形成了后端数据层的一个闭环，所有模块都是由我们团队负责的，这样就减少了沟通，并让DBA对整个业务架构有更深层次的把控。
>
> 另外除了经典的MySQL和Redis之外，我们还自研了一些周边的中间件系统，比如Redis中间件**Tribe**，以及异构数据同步的**Databus**等。
>
> **壹佰案例：** **微博的什么业务存在NoSQL上，什么业务存在MySQL上，这样做的选择是什么？**
>
> **肖鹏：** **在技术选型上，主要还是根据业务的具体场景**。一般来说**对于高并发低容量的（尤其是对响应时间非常敏感的业务），我们会建议使用Redis**，然后由于我们内部的Redis进行了自定制，可以作为cache也可以作为storage，所以会根据业务的整个架构决定是当cache使用，还是当storge使用。
>
> 对于其它的我们都会建议使用MySQL，毕竟**MySQL是一个相当通用的数据库**，而**对于一些类似于日志类的多写少读，并且预估容量会比较大的，我们会使用TokuDB引擎来解决。**
>
> 最近，由于Redis的数据结构对于开发非常简单易用，所以我们很多的需求都转向了Redis，我们也在调研内存落地到SSD盘上的解决方案。
>
> **哪些信息存MySQL？哪些存NoSQL？用的是哪种NoSQL？**
>
> 这就涉及一个分层存储的问题了，目前我们主流是 MySQL + Redis+mc，mc 和 Redis 都用来抗热点和峰值，而 MySQL 则为数据落地，保障最终有原始数据可查。大部分请求会到 mc 或者 Redis 层就返回了，只有不到 1% 的数据会到 MySQL上。
>
> 当然也有特殊的，比如之前说的计数服务，我们就把 rediscounter 当初落地存储使用，后面并没有在 MySQL 中再存储一份数据了。
>
> 最后，个人认为 NoSQL 最大的优势是开发的便捷性，开发人员使用 NoSQL 会比使用 MySQL 更简单，因为就是 KV 结构，所有查询都是主键查询，不用考虑 index 优化的问题，也不用考虑如何建表，这对于现在的互联网速度来说是有致命诱惑的。



## 3. mysql细谈

>  参考：
>
> - https://segmentfault.com/a/1190000004554857

微博到今天已经有6年了，新浪的MySQL集群结构主要经历了3次重大的变化。

### **3.1第一阶段：初创阶段**

初期微博作为一个内部创新产品，功能比较简洁，而数据库架构也采用1M2S1MB的标准结构，按照读写分离设计，主库承担写入，而从库承担访问。如果访问压力过大，可以通过扩容从库的数量获得scale out的能力。

### **3.2第二阶段：爆发阶段**

微博上线之后，随着用户活跃度的增加，数据库的压力也与日俱增。我们首先通过采购高性能的硬件设备来对单机性能进行scale up，以满足支撑业务的高速发展的需求。然后，通过使用高性能设备争取来的时间对微博进行整体上的业务垂直拆分，将用户、关系、博文、转发、评论等功能模块分别独立存储，并在垂直拆分的基础上，对一些预期会产+生海量数据的业务模块再次进行了二次拆分。

以博文为例，博文是微博用户主要产生的内容，可以预见会随着时间维度不断增大，最终会变得非常巨大。如何在满足业务性能需求的情况下，尽可能使用较少的成本存储，这是我们面临的一个比较有挑战的问题。

- 首先，我们将索引同内容进行了拆分，因为索引所需存储的空间较少，而内容存储所需空间较大，且对这两者的使用需求也不尽相同。
- 然后，分别对索引和内容采用hash，然后再按照时间维度拆分的方式进行水平拆分，尽量保障每张表的容量在可控范围之内，保障查询的性能指标。
- 最后，业务先通过索引获得实际所需内容id，然后再通过内容库获得实际的内容，并通过部署memcached来加速整个过程。虽然看上去步骤变多，但是实际效果完全可以满足业务需求
  - ![图片描述](https://segmentfault.com/img/bVtg4M)

### **3.3 第三阶段：沉淀阶段**

在上一个阶段，微博的数据库经历了很多的拆分改造，这也就直接造成了规模的成倍增长，而随着业务经历了高速增长之后，也开始趋于稳定。在这个阶段，我们开始着重进行自动化的建设。将之前在快速扩张期间积攒下来的经验转变为自动化工具，对外形成标准化和流程化的平台服务。我们相继改造了备份系统、监控系统、AutoDDL系统、MHA系统、巡检系统、慢查系统、maya中间件系统等。为了提高业务使用效率和降低沟通成本，相对于内部管理系统而言，我们重新开发了iDB系统对数据库平台的用户使用。通过iDB系统，用户可以便捷地了解自己业务数据库的运行状态，并可以直接提交对数据库的DDL修改需求。DBA仅需点击审核通过，即可交由Robot在线上执行，不但提高了工作效率，也提高了安全性和规范性。

### 第四阶段

随着业务的发展，会遇到越来越多的场景，我们希望可以引进最适合的数据库来解决场景问题，比如PostgreSQL、SSDB等。同时，利用MySQL新版本的特性（比如5.7的并行复制、GTID、动态调整BP），不断优化现有服务的性能和稳定性。

另外，对于现有的NoSQL服务，推进服务化，通过使用proxy将存储节点进行组织之后对外提供服务。对外降低开发人员的开发复杂度和获取资源的时间，对内提高单机利用率并解决资源层横向扩展的瓶颈问题。

同时，尝试利用各大云计算的资源，实现cache层的动态扩缩容；充分利用云计算的弹性资源，解决业务访问波动的问题。

## 4. Redis细谈

> 参考 
>
> https://www.zhihu.com/search?type=content&q=redis
>
> https://weibo.com/ttarticle/p/show?id=2309403978986932807291
>
> https://www.infoq.cn/article/weibo-relation-service-with-redis/

### **4.1 使用 Redis 的常见问题(可不讲)**

- 缓存和数据库双写一致性问题
- 缓存雪崩问题
- 缓存击穿问题
- 缓存的并发竞争问题

### 4.2 redis引入前期

2009 年微博刚刚上线的时候，微博关系服务使用的是最传统的 `Memcache+Mysql` 的方案。`Mysql` 按` uid hash` 进行了分库分表，表结构非常简单：

| tid     | fromuid  | touid    | addTime    |
| ------- | -------- | -------- | ---------- |
| 自增 id | 关系主体 | 关系客体 | 加关注时间 |

业务方存在两种查询：

- 查询用户的关注列表：`select touid from table where fromuid=？order by addTime desc`
- 查询用户的粉丝列表：`select fromuid from table where touid=？order by addTime desc`

两种查询的业务需求与分库分表的架构设计存在矛盾，最终导致了冗余存储：以` fromuid` 为 hash key 存一份，以` touid` 为 hash key 再存一份。`memcache key` 为` fromuid.suffix` ，使用不同的 suffix 来区分是关注列表还是粉丝列表，cache value 则为 `PHP Serialize` 后的 Array。后来为了优化性能，将 value 换成了自己拼装的 byte 数组。



### 4.2 微博Redis 6年变革

微博是从 **2010 年开始引入  Redis** .  2011 年微博进行平台化改造过程中，业务提出了新的需求：在核心接口中增加了“判断两个用户的关系”的步骤，并增加了“双向关注”的概念。因此两个用户的关系存在四种状态：关注，粉丝，双向关注和无任何关系。为了高效的实现这个需求，平台引入了`Redis` 来存储关系。

现在 Redis 已经**广泛应用于微博的多个业务场景，如关系、计数、通知提醒等**，目前` Redis` 集群存储超过百亿记录，每天上万亿的读取访问。随着业务的快速发展，我们在使用过程中碰到的问题及解决方法给大家做一个分享。主要包括以下方面： 实现机制高可用、业务极致定制以及服务化。

##### 4.2.1 Redis 2.0 时代（2010 - 2011）-实现机制高可用优化

- **持久化问题**

在我们大多数业务场景中 Redis 是当做存储来使用，会开启持久化机制。线上采用单机多实例的部署结构，服务器的内存使用率也会比较高。由于官方版本触发` bgsave` 和 `bgrewriteaof `操作的时间点是不可控的，依赖于相关的配置项和业务的写入模型，因此可能会出现单机部署的多个 Redis 实例同时触发 `bgsave` 或 `bgrewriteaof` 操作，这两个操作都是通过 fork 出一个子进程来完成的，由于 copy-on-write 机制，可能会导致服务器内存很快耗尽， Redis 服务崩溃。

此外在磁盘压力较大时（生成 rdb、aof 重写），对 aof 的写入及 fsync 操作可能会出现阻塞，虽然从 2.4 版本开始 fsync 操作调整到 bio 线程来做，主线程 aof 的写入阻塞仍会导致服务阻塞。

- **主从同步问题**

为了提高服务可用性，避免单点问题，我们线上业务 Redis 大多采用主从结构部署。官方版本的主从同步机制，在网络出现问题时（如瞬断），会导致主从重新进行一次全量复制。对单个端口来说，如果数据量小，那么这个影响不大，而如果数据量比较大的话，就会导致网络流量暴增，同时 slave 在加载 rdb 时无法响应任何请求。当然官方 2.8 版本支持了 psync 增量复制的机制，一定程度上解决了主从连接断开会引发全量复制的问题，但是这种机制受限于复制积压缓冲区大小，同时在主库故障需要执行切主操作场景下，主从仍然需要进行全量复制。

- **版本升级及管理问题**￼

早期 Redis 版本运行不够稳定，经常需要修复 bug、支持新的运维需求及版本优化，导致版本迭代很频繁。官方版本在执行升级操作时，需要服务重启，我们大多数线上业务都开启了持久化机制，重启操作耗时较长，加上使用 Redis 业务线比较多，版本升级操作的复杂度很高。由于统一版本带来的运维工作量实在太高，线上 Redis 版本曾经一度增加到十几个，给版本管理也带来很大的困难。

为了解决以上问题我们**对 Redis 原生实现机制做了以下优化**：

**1. 对于持久化机制，采用 rdb（快照（snapshotting）持久化） + aof（append-only file） 的持久化方式。**

`aof` 文件按固定大小滚动，生成 `rdb `文件时记录当前` aof` 的 position，全量的数据包含在 rdb 和所记录位置点之后的 `aof` 文件，废弃 aof 重写机制，生成` rdb` 后删除无效的 `aof` 文件；增加了定时持久化操作的配置项 `cronsave`，将单机部署的多个 Redis 实例的持久化操作分散在不同的时间点进行，并且错开业务高峰；将对 aof 的写入操作也放到 bio 线程来做，解决磁盘压力较大时 Redis 阻塞的问题。

**2. 对于主从同步机制，借鉴 MySQL 的复制机制并做了简化。**

使用` rdb + aof` 的方式，支持基于` aofpositon `的增量复制。从库只需与主库进行一次全量同步同步，后续主从连接断开或切主操作，从库都是与主库进行增量复制。

![img](https://ww2.sinaimg.cn/large/006kbv1Jgw1f47fmzk14uj312u0icn0r.jpg)





对于版本升和管理级的问题， Redis 的核心处理逻辑封装到动态库，内存中的数据保存在全局变量里，通过外部程序来调用动态库里的相应函数来读写数据。版本升级时只需要替换成新的动态库文件即可，无须重新载入数据。通过这样的方式，版本升级只需执行一条指令，即可在毫秒级别完成代码的升级，同时对客户端请求无任何影响。

![img](https://ww2.sinaimg.cn/large/006kbv1Jgw1f47fnwujqmj312o0cl40p.jpg)

￼￼￼除了以上几点，也做了很多其它的优化，如主从延迟时间检测，危险命令认证等。通过逐步的优化，内部的 Redis 版本也开始进入稳定期，应用规模也在持续的增加。

##### 4.2.2 **业务极致定制化时代（2012 - 2013）**

**RedisCounter / LongSet**

在某些特定的业务场景下，随着业务规模的持续增加， Redis 的使用又暴露出来一些问题，尤其是**服务成本**问题。为此结合特定的业务场景我们对 Redis 做了一些定制的优化。这里主要介绍一下在关系和计数两个业务场景下做的定制优化。

- **关系**

  微博关系业务包含添加、取消关注，判断关注关系等相关的业务逻辑，引入 Redis 后使用的是 hash 数据结构，并且当作存储使用。但是随着用户规模的快速增长，关系服务 Redis 容量达到十几 TB，并且还在快速的增长，如何应对成本压力?

  **为了解决服务成本问题，我们把 Redis 的角色由 storage 调整为 cache。**

  这是因为随着用户数量的增长，业务模型由初期的热点数据不集中已经转变为有明显的冷热之分。对于关注关系变更、判断关注关系，hash 数据结构是最佳的数据结构，但是存在以下问题：

  1.  cache miss 后回写关注列表性能差，对于关注数较多的微博会员，回写操作耗时可达到 10ms，这对于单线程的 Redis 来说是致命的；
  2. Redis hash 结构的内存使用率不高，要保证 cahce 的命中率所需的 cache 容量仍然是很大的。

  于是，我们**定制了 `longset `数据结构**，它是一个“固定长度开放寻址的 hash 数组”，通过选择合适的 hash 算法及数组填充率，可实现关注关系变更及判断的性能与原生 Redis hash 相当，同时 cache miss 后通过 client 重建 longset 结构，实现 O(1) 复杂度回写。

  **通过定制 longset 数据结构，将关系 Redis 内存占用降低了一个数量级（小编：这该节约了多少服务器……发奖金了吗？），同时保证了服务性能。**

![img](https://ww2.sinaimg.cn/large/006kbv1Jgw1f47fp3ydf2j313d0cp40c.jpg)

- **计数**

  微博有很多计数场景，如用户纬度的关注数、粉丝数，微博纬度的转发数、评论数等。计数作为微博中一项很重要的数据，在微博业务中承担了很重要的角色。为更好的满足计数业务需求，我们基于 Redis 定制了内部的计数服务。

  原生 Redis 为了支持多数据类型，需要维护很多指针信息，存储一份业务计数要占到约 80 个字节，内存利用率很低。为此我们定制了第一版计数器 Redis counter，**通过预先分配内存数组存储计数，并且采用 doublehash 解决冲突**，减少了原生 Redis 大量的指针开销。通过以上优化将内存成本降低到原来的 1/4 以下。（小编：又节约了 3 / 4 服务器……）

  随着微博的发展，微博纬度的计数不断增加，在原来的转发数、评论数基础上，又增加了表态数，2013 年还上线了阅读数。 Redis counter 已不能很好的解决这类扩展问题：

  1. 存储单条微博相关的计数，需要重复存储微博 mid 信息，并且数据全部存储在内存，服务成本较高；
  2. 获取单条微博全部的计数，需要调用多次计数接口，对服务端压力很大。

  为此我们又设计了改进版的计数器 CounterService，增加如下特性：

  - Schema 支持多列：支持动态加列，内存使用精简到 bit

  - **冷热数据分离**：频繁访问的热数据存储在 memory，访问较少的冷数据存储在磁盘，降低服务成本

  - **LRU 缓存冷数据**：增加 LRU 模块，缓存访问到的冷数据，保证冷数据的访问性能。

  - **异步 IO 线程访问冷数据**：避免冷数据的访问影响服务的整体性能

    ￼￼通过以上的定制优化，我们从根本上解决了计数业务的成本及性能问题。![img](https://ww1.sinaimg.cn/large/006kbv1Jgw1f47giuuhkjj30ux0oc429.jpg)

    除了以上关系、计数业务场景的定制优化，为了满足判断类业务场景需求，定制了 BloomFilter 服务；为了满足 feed 聚合业务场景需求，定制了 VerctorService 服务；为了**降低服务成本**，定制了 SSDCache 服务等。

##### **4.2.3 服务化时代（2014 -）**

随着微博业务的快速增长，Redis 集群规模也在持续增加，目前微博 Redis 集群内存占用数十 TB，服务于数百个业务线，Redis 集群的管理依然面临很多的问题。

- **数据迁移问题**

  随着时间推移，越来越多的业务由于数据量的增加，单端口到内存占用已经达到上限，**微博内部建议单端口内存不超过 20GB**，因此需要重新拆分端口，这就涉及到数据迁移，目前迁移操作是通过内部开发的一个迁移工具来完成的，迁移操作的成本相对较高。

- **数据路由问题**

  ￼目前的使用方式，需要在业务代码中实现数据路由规则，路由规则的变更需要重新上线代码，业务变更复杂度较高。同时节点配置采用 DNS 的方式，存在实时性和负载不均的问题，虽然使用过程中有对应的解决策略，但是需要一定的运维干预，运维复杂度较高。

- **HA 系统不成熟**

  当前的 HA 系统更多的是采用自动发现问题，手动确认处理的策略，没有实现真正意义的自动化，运维成本依然很高。

为了解决以上问题，我们**在 Redis 基础上实现服务化框架 `CacheService`**。

`CacheService` 最早是为了解决内部使用 `memcached` 遇到的问题而开发的服务化框架，主要包含以下几个模块：

- **配置中心 ConfigServer**

微博内部的配置服务中心，主要是管理静态配置和动态命名服务的一个远程服务，并能够在配置发生变更的时候实时通知监听的 ConfigClient。

- **资源层**

实际的数据存储引擎，初期支持 memcached，后续又扩展了 Redis、SSDCache 组件，其中 SSDCache 是为了降低服务成本，内部开发的基于 SSD 的存储组件，用于缓存介于 memory 和 DB 之间的 warm 数据。

- **代理层**

代理业务端的请求，并基于设定的路由规则转发到后端的 cache 资源，它本身是无状态的。proxy 启动后会去从 ConfigServer 加载后端 cache 资源的配置列表进行初始化，并接收 ConfigServer 的配置变更的实时通知。

- **客户端**

提供给业务方使用的 SDK 包，通过它不需要在业务代码中实现数据路由规则，业务方也无需关心后端 cache 的资源。只需要简单配置所使用的服务池名 group 和业务标识 namespace 即可使用 cache 资源，client 从 ConfigServer 获取 proxy 的节点列表，选择合适的 proxy 节点发送请求，支持多种负载均衡策略，同时会自动探测 proxy 节点变更。

- **集群管理系统` ClusterManager`**

管理集群中各个组件的运行状态以保证业务的 SLA 指标，当出现异常时会自动执行运维处理。同时配置变更、数据迁移等集群操作也都是由它来负责。￼￼

![img](https://ww4.sinaimg.cn/large/006kbv1Jgw1f47h7a6hi9j30xr0praf5.jpg)

​	为支持 Redis 服务化，在服务化框架扩展支持了 Redis proxy，同时为了实现在线数据迁移，参照 Redis cluster 的设计思想，对内部 Redis 存储做了改造，支持 slot 数据分片，数据迁移操作由 ClusterManager 组件执行，完成 slot 的重新规划及数据迁移。此外还支持 Redis 的 failover 机制，在master 或 slave 节点故障时会自动执行容错处理。

##### **4.2.4 Redis中间件**(2015)

2015年，我们自研的Redis中间件`tribe`系统完成了开发和上线。tribe采用有中心节点的proxy架构设计，通过configer server管理集群节点，并借鉴官方Redis cluster的slot分片设计思路来完成数据存储。最终实现了路由、分片、自动迁移、fail over等功能，并且预留了操作和监控的API接口，以便同其他的自动化运维系统对接。

![图片描述](https://segmentfault.com/img/bVtg4U)

**Databus**

由于我们先有MySQL后有Redis和HBase等数据库，所以存在一种场景，就是目前数据已经写入到MySQL中，但是需要将这些数据同步到其他数据库软件中。为此我们开发了Databus，可以基于MySQL的binlog将数据同步到其他异构的数据库中，并且支持自定义业务逻辑。目前已经实现了MySQL到Redis和MySQL到HBase的数据流向，下一步计划开发Redis到MySQL的数据流向。

![图片描述](https://segmentfault.com/img/bVtg4Y)





## 具体业务示例





## 参考文档

