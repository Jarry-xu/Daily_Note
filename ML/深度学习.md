---
typora-root-url: ./assets
---

[TOC]



# Andrew Ng 深度学习系列课程

## 第一部分 神经网络和深度学习

### 第一周 深度学习概论 略

### 第二周 神经网络基础

#### 二分类问题

Loggistic 分类（regression）

逻辑斯提 运用 sigmoid函数 将数值映射到0-1之间，变成一种概率值表示，表示分类为某种类的概率   将 b塞入W中比较方便表示

Cost func 是训练的依据，训练要往降低cost func的方向去

这里损失函数不是误差平方函数 因为是非凸的    ，我们最好需要找凸函数作为损失函数

Loss定义如下： 凸函数   

Loss的直觉化思考   二分类有效

将y=0或者1统一到一个表达式里面

![1546677597583](/1546677597583.png)

$L(\hat y,y)=-(ylog\hat y+(1-y)log(1-\hat y))$

Costfunc 定义如下：

$J(W,b)=\frac{1}{m}\sum_{i=1}^{n}L(\hat y^{(i)},y^{(i)})$

那么如何通过训练数据进行参数学习呢？

#### Gradient Descent 

初始化参数

通过计算梯度，参数向梯度相反的方向前进一小步，导致J往变小的方向发展

使用循环来进行梯度下降太慢了，所以用矩阵来表示就很方便！

向量化比for循环快三百倍！所以能不用循环就不用循环



### 神经网络

每一层的参数矩阵W为（m,n）维度，m为这一层的节点数，n为上一层的节点数,每一行都与上一层的所有节点线性组合构成这一层的某个的值，进行向量化的时候，其实W是不变的，只是X由列向量变成了列向量的堆叠

![1546753894635](/1546753894635.png)

![1546754063800](/1546754063800.png)

#### 参数随机化

初始化参数不应该太大，否则会落在sigmoid平缓的部分，梯度很小，学习很慢 



### 第三周 深度神经网络



#### 深度神经网络的前向传播  多层传播

单个训练样本以及多个训练样本的向量化表示

![1546757228570](/1546757228570.png)

为什么深层表示有效

1. 多层比浅层需要的节点要少，因为浅层所需要的参数组合很多 2^(n-1)
2. 抽取局部特征进行组合，拼接成整体特征 



## 第二部分：提升深度神经网络

#### 正则化   减少过拟合   但是需要尝试很多的λ值

L1正则会是稀疏的    

​	使用Dropout来进行正则化  每个权重不会被赋予过大的值

​	early stopping

​	缺点：提前停止降低J，J可能不够小



数据归一化的作用    数据规约到一个合适的范围 使得梯度下降更容易

梯度消失和梯度爆炸 都会导致训练难以进行



Mini-batch 使用大小为m的训练样本多次执行梯度下降，速度比整个batch要快

m=1  SGD

m=整个数据集的数量      batch train



第三部分  卷积神经网络

准确 来说 做的不是卷积而是互相关操作  因为没有进行翻转的操作

 padding p 步长s 

![1546848694961](/1546848694961.png)

LeNet-5      炉温

6个5x5过滤器->maxpooling2x2->16个5x5  ->maxpooling2x2->flattern->dense 84->softmax   

AlexNet    使用ReLu

![1546851819469](/1546851819469.png)

vgg-16   参数太多

![1546852059707](/1546852059707.png)

ResNetwork   152层

残差网络  层输入可以进行跳跃连接

Inceoption

![1546864696346](/1546864696346.png)