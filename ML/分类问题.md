---
typora-root-url: assets\
---

[TOC]

线性回归不适合进行分类

通过将回归值进行阈值判断来进行离散化

Logistic 分类实际上是一种软分类，输出属于不同类别的概率

# Logistic分类

## Logistic 二分类问题

### Logistic func

定义映射函数$g(z)$为：
$$
g(z)=\frac{1}{e^{-z}+1}\\
其中z=\theta^Tx
$$
定义假设$h_\theta(x)$为：
$$
h_\theta(x)=g(\theta^Tx)
$$
h(x)的含义表示为分成某类的概率为多大

![1553481821199](1553481821199.png)

从上我们可以得到y=0和y=1在给定x情况下的概率输出

### 代价函数是什么样子的？

$$
Cost(h_\theta(x_i),y_i)=\frac{1}{2}(h_\theta(x_i)-y_i)^2
\\
J(\theta)=\sum_{i=1}^{m}Cost(h_\theta(x_i),y_i)
$$

这是一个非凸函数，所以寻找另外一个代价函数：

### Cost func 改进

![1553482559418](1553482559418.png)

Cost func 压缩成一个公式：
$$
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))\\
$$
理论证明这是一个凸函数

log函数是一个平滑单调的函数，且趋近0的时候，数值趋于无穷大，惩罚很大

那么
$$
接着（4）进行计算\\
cost(h_\theta(x),y)=-ylog(h(x)/1-h(x))-log(1-h(x))
化简之后变为：
\\
-y\theta^TX+log(e^{\theta^TX}+1)\\
对\theta进行求导，其中y是一个标量,X和\theta都是向量，
\frac{\partial cost(h_\theta(x),y)}{\partial \theta}=-yX+h(x)X
$$
那么向量化的梯度下降算法应该是如下形式：m为样本数量     n为样本维度
$$
\theta=\theta-\frac{\alpha}{m} \sum_{i=1}^{m}(h(x_i)-y_i)x_i
\\再进一步:\\
\theta=\theta-\frac{\alpha}{m} X(h(X)-Y)
$$

### 从概率论的角度去理解Logistic分类

我们训练的目的是为了达到模型预测所有样本成功的概率越大越好，假设预测是一个彼此独立的事件

设$P(Y=1|x)=g(x),P(Y=0|x)=1-g(x)$

那么似然函数为：


$$
\prod_{i=1}^{m}[g(x_i)]^{y_i}[1-g(x_i)]^{1-y_i}
$$
写成上面这个式子其实是把判断y为零为一统一了进去，当y=1时乘左边的式子，y=0乘右边的式子

对这个似然函数的对数：
$$
L（\theta）=\sum_{i=1}^m y_ilog(g(x_i))+(1-y_i)log((1-g(x_i)))
\\化简可以得到同之前差不多的式子\\
\sum_{i=1}^m[y_i(\theta^Tx_i)-log(1+e^{\theta^Tx_i})]
$$
然后同样是用梯度下降的方法去求解

### 更多优化手段

![1553502339100](./1553502339100.png)

## Logistic多分类问题

### one vs all

把一个类作为正类，其他的作为负类

据此训练出多个h(x)，最后把输出概率最大的那个分类作为其类标



# Logistic的局限

Logistic也只能处理线性问题，非线性无力！


