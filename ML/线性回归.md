---
typora-root-url:assets\
---

[TOC]

回归是对于连续值的预测 分类是对离散值的预测

## 单变量线性回归

前置条件： 共有m条数据

定义 假设H（x）为：

$h(x)=w_0+w_1x_1$        

定义代价函数$J(w_0,w_1)$为:  用平方误差作为衡量标准

$$J(w_0,w_1)=\frac{1}{2m}\sum_{i=1}^{m}(y_i-h(x_i))^2$$

目标函数变为

$min J(w_0,w_1)$

那么如何去做优化呢？

1. 梯度下降方法   每次参数更新都顺着梯度的反方向进行，这样不断接近局部最优解，注意，不一定是全局最优解

   $w_0=w0-α\frac{\partial} {\partial w_0}J(w_0,w_1) $

​	$w_1同理$

​	这两个应该是同时更新的，根据具体的代价函数，在具体代码实现中也是不一样的

​	ps: 代价函数很特殊的一点在于，**他是一个凸函数**，它只有一个全局最小点，所以优化结果只有一种。

![1552657515283](/1552657515283-1553480001910.png)

​	梯度下降方法进化：随机梯度下降

2. 使用线性代数的方程组去解方程

   在多元线性回归中讲解

## 多元线性回归

设有n个特征

其实求解过程跟单元变量差不多，只是需要求解的参数变多了

## 多元线性回归的向量化表示

### 向量化梯度下降

将参数组成一个列向量W
$$
W=\left[ \begin{array}{c}

w_0\\

w_1\\w_2 \\
.. \\
w_n

 \end{array} 
\right ]
$$
X是一个nxm的矩阵，每一列代表一组数据
$$
X=\left[ \begin{array}{c}

x_{10}  \quad x_{20} \quad...\quad x_{m0}\\
x_{11} \quad x_{21}\quad... \quad x_{m1}\\
x_{12}  \quad x_{22}\quad... \quad x_{m2}\\
.. \\
x_{1n}  \quad x_{2n}\quad... \quad x_{mn}\\

 \end{array} 
\right ]
$$
其实第一行都为1

那么损失函数J(W)

$J(W)=\frac{1}{2m}||W^TX-Y||_2^2$

那么使用梯度下降更新公式就变为了

$w_0=w0-α\frac{\partial} {\partial w_0}J(W) $

同理可以更新其他参数$w_1,w_2,...,w_n$

### 正规方程：

![1553480600070](/1553480600070.png)

令上式等于0可得到下面的公式

$ W=(X^TX)^-1X^Ty$

#### 特殊情形

如果$X^TX$不可逆

1. 包含冗余的特征
2. 特征太多或者使用正则化
3. 老实使用梯度下降

### 关于梯度下降和正规方程的抉择

n<10000用正规方程，大于10000用梯度下降

![1552728182810](/1552728182810.png)

#### 

## 非线性函数回归：多项式回归  

 h(X)不再是线性公式，存在多次方以及变量之间的交叉乘积



## 额外引出的内容：

1. 归一化 消除特征之间因为数值量级带来的影响

   1. 均值归一化

   $x=\frac{x-u}{s_1}$   u为均值，s1为标准差