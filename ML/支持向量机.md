---
typora-root-url: assets\

---

[TOC]



# SVM

## hard-margin

大前提：数据是线性可分的！

### 从Logistic的目标函数开始

![1553845532981](/1553845532981.png)

将代价函数简化为上面两个图所示，他们在Z大于等于1或者小于等于-1的时候都等于0 其中Z=$\theta^Tx$

于是目标函数变为了
$$
\min_\theta\frac{1}{m}[\sum_{i=1}^my_icost_1(\theta^Tx)+(1-y_i)cost_0(\theta^Tx)]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
$$
其实m是个常数，我们在优化过程中可以省略，于是变为
$$
\min_\theta[\sum_{i=1}^my_icost_1(\theta^Tx)+(1-y_i)cost_0(\theta^Tx)]+\frac{\lambda}{2}\sum_{j=1}^n\theta_j^2
$$
在Logistic中记左边的代价为A右边为B，那么整体就为$A+\lambda B$

在SVM中，我们记为CA+B 其实可以把C看做$\lambda$的倒数

于是优化目标最终演化为了：
$$
\min_\theta C\sum_{i=1}^m[y_icost_1(\theta^Tx)+(1-y_i)cost_0(\theta^Tx)]+\frac{1}{2}\sum_{j=1}^n\theta_j^2
$$
![1553846551222](/1553846551222.png)

加入把C设置的非常大，那么非常希望第一项A=0，那么我们希望上图中的结果，y=1，Z≥1，y=-1,z≤-1。



通过加入约束条件我们得到了一个新的目标函数：
$$
\min_\theta\frac{1}{2}\sum_{j=1}^n\theta_j^2
\\s.t \quad \theta^Tx\ge1 \quad if \quad y=1\\
\quad \theta^Tx\le-1 \quad if \quad y=0\\
$$
$\theta^Tx是向量\theta和某个样本的向量表示的内积
  , 用范数改写一下变为\\$
$$
\min_\theta\frac{1}{2}||\theta||^2
\\s.t \quad p\cdot||\theta||\ge1 \quad if \quad y=1\\
\\s.t \quad p\cdot||\theta||\le-1 \quad if \quad y=0\\
$$


### 直观与数学理解

SVM不是为了得到刚好分类的结果，而是为了得到更好的边界，因此SVM会选择距离正负类最大间距的超平面 ，如果C非常大，非常容易受到离群点的影响，但如果C不是很大的话，对于离群点的处理就会更好

![1553847004019](/1553847004019.png)



主观上，找到一个离两边距离都比较大的决策边界，具有更好的泛化性能

![1553779030755](/1553779030755.png)

从数学上：

决策边界的垂直方向正好是向量$\theta$ 的方向，要满足约束条件的条件下减少目标函数，那就需要训练集在$\theta$ 上面的投影比较大，从而使得上图中简化的目标函数越变越小，最后能找到一个合适的决策边界





### 另外一个角度

几个loss函数

1. Square_loss  (y*h(x)-1)^2   
2. sigmoid loss 
3. hinge loss

目标是找到最宽的margin 

![1553849188386](/1553849188386.png)

h(x)输出的结果就是1或者-1。$yh(x)>0$说明正确分类

在这里W不包含$w_0$于是$h(x)=sign(W^Tx+b)$

![1553849723702](/1553849723702.png)

![1553849907197](/1553849907197.png)

因为要求都分对，所以绝对值可以去掉，变为$y_n(W^Tx+b)$

由于常数项系数任意放缩不会影响到取最小值的结果，所以

![1553850119288](/1553850119288.png)

所以问题简化为求上图中的式子，要求超平面里面最小的距离为1.

1. 把约束条件放松，而且可以保证放松了条件之后，最优解仍然在原来的位置。
2. 把max改为min W取倒数，去掉了分数

![1553850341138](/1553850341138.png)

在有约束条件下的优化问题，不容易用梯度下降的方法去求解

观察有以下特征：

1. 这种属于凸优化问题（至于为什么我也不知道）

2. 有线性约束条件

所以属于二次规划问题

![1553850884789](/1553850884789.png)



### SVM的有效性

to be complete



### SVM的对偶形式

其目的是把有约束问题变为无约束问题

原本的SVM目标函数：有n+1个变量，N个约束条件，n为特征数量,N为样本数量

![1553922467707](/1553922467707.png)

使用拉格朗日乘法，变成一个无约束的式子
$$
L(b,W,\alpha)=\frac{1}{2}W^TW+\sum_{n=1}^N \alpha_n(1-y_n(W^Tx_n+b))
$$
于是SVM转换为：

![1553922134788](/1553922134788.png)

注意观察，$1-y_n(*)\le0$，在给定b,w下，要找出里面的那个最大值，那么$\alpha$的值应该越小越好，如果取零，那么就正好取得最大值$\frac{1}{2}W^TW$与原来的式子是一样的。

在这样一些给定的b,w中，找出值最小的那一组参数。

下图说明，对偶问题找到了原问题的一个下界

![1553922628457](/1553922628457.png)

下面需要证明对偶问题和原问题不只是弱对偶问题，而满足强对偶问题条件：

1. 凸优化
2. 线性可分
3. 有线性约束条件

![1553923039486](/1553923039486.png)

![1553923199019](/1553923199019.png)

这就引出了KKT的条件：

1. 原有问题的约束条件
2. 对偶的原本条件：$\alpha_n \ge0$
3. 对偶问题的求偏微分过程中获得的约束条件 
4. 原问题要取到$\frac{1}{2}W^TW$这个值所要满足的条件 ![1553923468907](/1553923468907.png)

把求最大化变为最小化

![1553923878122](/1553923878122.png)

变为二次规划的问题

![1553924423680](/1553924423680.png)

Q矩阵为NXN的大小

![1553924722787](/1553924722787.png)

如果$\alpha_n>0$，那么这个点一个支撑向量，如果$\alpha_n =0$那么该点可能是，也可能不是支撑向量

![1553924925531](/1553924925531.png)



### 原始与对偶的适用范围

![1553925193598](/1553925193598.png)

**其实在对偶里面计算也跟特征的数量有关，它被内包含在Zn中**

![1553925229620](/1553925229620.png)



那么真的要解决依赖于$\tilde{d}$的问题，需要引入kernel method



### kernel method

在原始空间做内积，再转换到高维空间，省略了更高的维度的大计算量

依据原理是
$$
K_\phi(x_1)^TK_\phi(x_2)=K_\phi(x_1^Tx_2)
$$
![1553926016597](/1553926016597.png)

不同的kernel method    不同的kernel 对边界的形成有影响

- linear kernel     数据线性可分

- Polynomial        边界线是非线性的，模型复杂取决于Q，  缺点   容易受到多次方的影响  Q只能比较小，需要选择三个参数

- 无穷维

  - 高斯kernel    RBF 更一般的形式是$K(x,x')=exp(-\gamma||x-x'||^2 )$

  -  速度比较慢，只需要选择一个参数

    ![1553932949731](/1553932949731.png)

  $\gamma 变大了的话$容易过拟合，因为高斯分布变尖了

  ![1553933323903](/1553933323903.png)



### 吴恩达课程中的 kernel

核函数本质上是一种相似度量

![1554194610443](/1554194610443.png)

通过计算与标记点之间的距离，最后加权得到一个分数，通过分数的正负判断类别。

而距离的度量函数就是核函数

## soft-margin

使用核函数的SVM仍然会过拟合



$\xi_n$表示违反的程度，不再用上面的那种判断式子只能判断个数，不能计算程度

![1553936021175](/1553936021175.png)



利用lagerange求解对偶问题

![1553936684335](/1553936684335.png)



![1553936756102](/1553936756102.png)

![1553936768701](/1553936768701.png)

其实和hard-svm差不多，唯一的差别是$\alpha_n\le C$

求$\xi_n$

![1553939357416](/1553939357416.png)

**求b**

1. 如果$0<a_n<C则\xi_n=0$   这上面的点都是边界上的点    b和hard-svm的求法一样
2. 如果$\alpha_n=0,则\xi_n=0$ 这些点可能是边界点可能是正确分类的点
3. 如果$\alpha_n=C$，分三种情况
   1. 第一种$\xi_n=0$,这时候是边界上的点
   2. $0<\xi_n<1$,分类正确，在分离超平面与间隔线之间
   3. $\xi_n=1$ 点在分离超平面上
   4. $\xi_n>1$ 点位于误分的一侧

第二第三种情况,$b=y_s-y_s\xi_s-W^Tz_s$ 就是用随便一个支持向量就能得到b



### 参数选择

根据CV结果来选择合适的参数组合

验证误差的边界

因为只有支持向量上才可能分错，其他的都分对了，误差为零

![1553938294872](/1553938294872.png)







## Sum up

在实际应用中，一般都会使用soft-margin+kernel-method



SVM本质上也是一个正则化过的模型

![1553939710640](/1553939710640.png)

### Logistic 与 SVM的选择

![1554196075283](/1554196075283.png)



Kernel Logistic regression

![1554205478064](/1554205478064.png)

SVM的误差是0/1误差的上限

![1554205890174](/1554205890174.png)

SVM其实与正则化的logistic



# SVR 

支持向量回归与线性回归的差别：

支持向量回归允许h(x)与y之间最多有$\epsilon$的偏差，即只有$|h(x)-y|>\epsilon$的时候才计算损失，因此在h(x)为中心，构建了一条宽度为$2\epsilon的间隔带$。

因此目标函数变为
$$
\min_{W,b}\frac{1}{2}||w||^2+C\sum_{i=1}^{m}l_\epsilon(h(x_i)-y_i)
\\
l_e(z)=\{_{|z|-\epsilon \quad otherwise} ^{0\quad |z|\le\epsilon}
$$
